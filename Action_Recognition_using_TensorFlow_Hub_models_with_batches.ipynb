{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDq0CIKc1vO_"
      },
      "source": [
        "# Action Recognition with an Inflated 3D CNN\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6W3FhoP3TxC"
      },
      "source": [
        "This notebook is inspired from [here](https://www.tensorflow.org/hub/tutorials/action_recognition_with_tf_hub). I chose to follow this tutorial as tensorflow hub has a wide range of available pre trained models ready for use, able to be customised and fine tuned.\n",
        "\n",
        "The original tutorial used the i3d model trained on kinetics 400, I changed the model to use the i3d for kinetics 600, as I think that more labels would give a more confidence scoring system (closer to 100%) for predicted labels.\n",
        "\n",
        "I also wanted to use the UCF101 dataset because it was more established and has more papers around it.\n",
        "\n",
        "This exercise is run in Colab. Since no training is required, GPU need not be necessary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_0xc2jyNGRp"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mOHMWsFnITdi",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "!pip install -q imageio\n",
        "!pip install -q opencv-python\n",
        "!pip install -q git+https://github.com/tensorflow/docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "USf0UvkYIlKo",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "#@title Import the necessary modules\n",
        "# TensorFlow and TF-Hub modules.\n",
        "from absl import logging\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow_docs.vis import embed\n",
        "\n",
        "logging.set_verbosity(logging.ERROR)\n",
        "\n",
        "# Some modules to help with reading the UCF101 dataset.\n",
        "import random\n",
        "import re\n",
        "import os\n",
        "import tempfile\n",
        "import ssl\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# Some modules to display an animation using imageio.\n",
        "import imageio\n",
        "from IPython import display\n",
        "\n",
        "from urllib import request  # requires python3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IuMMS3TGdws7",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "#@title Helper functions for the UCF101 dataset\n",
        "\n",
        "# Utilities to fetch videos from UCF101 dataset\n",
        "UCF_ROOT = \"https://www.crcv.ucf.edu/THUMOS14/UCF101/UCF101/\"\n",
        "_VIDEO_LIST = None\n",
        "_CACHE_DIR = tempfile.mkdtemp()\n",
        "# As of July 2020, crcv.ucf.edu doesn't use a certificate accepted by the\n",
        "# default Colab environment anymore.\n",
        "unverified_context = ssl._create_unverified_context()\n",
        "\n",
        "def list_ucf_videos():\n",
        "  \"\"\"Lists videos available in UCF101 dataset.\"\"\"\n",
        "  global _VIDEO_LIST\n",
        "  if not _VIDEO_LIST:\n",
        "    index = request.urlopen(UCF_ROOT, context=unverified_context).read().decode(\"utf-8\")\n",
        "    videos = re.findall(\"(v_[\\w_]+\\.avi)\", index)\n",
        "    _VIDEO_LIST = sorted(set(videos))\n",
        "  return list(_VIDEO_LIST)\n",
        "\n",
        "def fetch_ucf_video(video):\n",
        "  \"\"\"Fetchs a video and cache into local filesystem.\"\"\"\n",
        "  cache_path = os.path.join(_CACHE_DIR, video)\n",
        "  if not os.path.exists(cache_path):\n",
        "    urlpath = request.urljoin(UCF_ROOT, video)\n",
        "    print(\"Fetching %s => %s\" % (urlpath, cache_path))\n",
        "    data = request.urlopen(urlpath, context=unverified_context).read()\n",
        "    open(cache_path, \"wb\").write(data)\n",
        "  return cache_path\n",
        "\n",
        "# Utilities to open video files using CV2\n",
        "def crop_center_square(frame):\n",
        "  y, x = frame.shape[0:2]\n",
        "  min_dim = min(y, x)\n",
        "  start_x = (x // 2) - (min_dim // 2)\n",
        "  start_y = (y // 2) - (min_dim // 2)\n",
        "  return frame[start_y:start_y+min_dim,start_x:start_x+min_dim]\n",
        "\n",
        "def load_video(path, max_frames=0, resize=(224, 224)):\n",
        "  cap = cv2.VideoCapture(path)\n",
        "  frames = []\n",
        "  try:\n",
        "    while True:\n",
        "      ret, frame = cap.read()\n",
        "      if not ret:\n",
        "        break\n",
        "      frame = crop_center_square(frame)\n",
        "      frame = cv2.resize(frame, resize)\n",
        "      frame = frame[:, :, [2, 1, 0]]\n",
        "      frames.append(frame)\n",
        "      \n",
        "      if len(frames) == max_frames:\n",
        "        break\n",
        "  finally:\n",
        "    cap.release()\n",
        "  return np.array(frames) / 255.0\n",
        "\n",
        "def to_gif(images):\n",
        "  converted_images = np.clip(images * 255, 0, 255).astype(np.uint8)\n",
        "  imageio.mimsave('./animation.gif', converted_images, fps=25)\n",
        "  return embed.embed_file('./animation.gif')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GM1hGNAqX5Pz"
      },
      "source": [
        "# Get the kinetics 600 label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pIKTs-KneUfz",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Get the kinetics-600 action labels from the GitHub repository.\n",
        "KINETICS_URL = \"https://raw.githubusercontent.com/tensorflow/models/f8af2291cced43fc9f1d9b41ddbf772ae7b0d7d2/official/projects/movinet/files/kinetics_600_labels.txt\"\n",
        "with request.urlopen(KINETICS_URL) as obj:\n",
        "  labels = [line.decode(\"utf-8\").strip() for line in obj.readlines()]\n",
        "print(\"Found %d labels.\" % len(labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBvmjVICIp3W"
      },
      "source": [
        "# Using the UCF101 dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V-QcxdhLIfi2",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Get the list of videos in the dataset.\n",
        "ucf_videos = list_ucf_videos()\n",
        "  \n",
        "categories = {}\n",
        "for video in ucf_videos:\n",
        "  category = video[2:-12]\n",
        "  if category not in categories:\n",
        "    categories[category] = []\n",
        "  categories[category].append(video)\n",
        "print(\"Found %d videos in %d categories.\" % (len(ucf_videos), len(categories)))\n",
        "\n",
        "for category, sequences in categories.items():\n",
        "  summary = \", \".join(sequences[:2])\n",
        "  print(\"%-20s %4d videos (%s, ...)\" % (category, len(sequences), summary))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3i5Mau3RGRiw",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# test visualization \n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "categories_distribution = {key: len(value) for key,value in categories.items()}\n",
        "plt.bar(range(len(categories_distribution)), list(categories_distribution.values()), align='center')\n",
        "plt.xticks(range(len(categories_distribution)), list(categories_distribution.keys()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0ZvVDruN2nU",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Get a sample cricket video.\n",
        "video_path = fetch_ucf_video(\"v_VolleyballSpiking_g01_c01.avi\")\n",
        "sample_video = load_video(video_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hASLA90YFPTO",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "sample_video.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "POf5XgffvXlD",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "i3d = hub.load(\"https://tfhub.dev/deepmind/i3d-kinetics-600/1\").signatures['default']\n",
        "\n",
        "# for future training if necessary\n",
        "model = tf.keras.Sequential([\n",
        "    hub.KerasLayer(i3d, input_shape=(None,224,224,3)),\n",
        "])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3mTbqA5JGYUx",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "def predict(sample_video):\n",
        "  # Add a batch axis to the sample video.\n",
        "  model_input = tf.constant(sample_video, dtype=tf.float32)[tf.newaxis,...]\n",
        "\n",
        "  logits = model(model_input)[0]\n",
        "  probabilities = tf.nn.softmax(logits)\n",
        "\n",
        "  print(\"Top 5 actions:\")\n",
        "  for i in np.argsort(probabilities)[::-1][:5]:\n",
        "    print(f\"  {labels[i]:22}: {probabilities[i] * 100:5.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ykaXQcGRvK4E",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "predict(sample_video)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmFh2iLpJom4"
      },
      "source": [
        "## Use this to test sample videos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vug62bN8MuoE",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "def download_random_video():\n",
        "    actions = list(categories.keys())\n",
        "    action = random.choice(actions)\n",
        "    index = random.randint(0,len(categories[action])-1)\n",
        "    return categories[action][index]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lpLmE8rjEbAF",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "random_video = download_random_video()\n",
        "print(random_video)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CHZJ9qTLErhV",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "video_path = fetch_ucf_video(random_video)\n",
        "sample_video = load_video(video_path)[:50]\n",
        "sample_video.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ZNLkEZ9Er-c",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "to_gif(sample_video)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yskHIRbxEtjS",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "predict(sample_video)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpK3WdMQZW3H"
      },
      "source": [
        "## Tried enhancements\n",
        "\n",
        "The idea is to cut down the 600 labels to only 101, to do this I trained an individual video through an added dense layer of the number of classes for the UCF101 dataset. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3D3dPUabZqvh",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "def download_random_video_with_label():\n",
        "    actions = list(categories.keys())\n",
        "    action = random.choice(actions)\n",
        "    index = random.randint(0,len(categories[action])-1)\n",
        "    return categories[action][index], actions.index(action)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YN4CIGCXOGym",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "new_model = tf.keras.Sequential([\n",
        "    hub.KerasLayer(i3d, input_shape=(None,224,224,3)),\n",
        "    tf.keras.layers.Dense(101),\n",
        "])\n",
        "\n",
        "new_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7nozSWrQZzUi",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "new_model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
        "    loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "    metrics=['accuracy'],\n",
        ")\n",
        "\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.TensorBoard(),\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ABDLMXheaFFe",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "NUM_EPOCHS = 20\n",
        "NUM_VIDEOS=100\n",
        "\n",
        "batch_videos = np.zeros((NUM_VIDEOS,48,224,224,3))\n",
        "batch_labels = np.zeros((NUM_VIDEOS,101))\n",
        "for j in range(NUM_VIDEOS):\n",
        "  video,index = download_random_video_with_label()\n",
        "  items = list(categories.keys())\n",
        "  \n",
        "  print(video,index,items[index])\n",
        "\n",
        "  video_path = fetch_ucf_video(video)\n",
        "  train_video = load_video(video_path)[np.newaxis,...]\n",
        "  batch_videos[j] = train_video[:,:48,:]\n",
        "  \n",
        "  label_list = [0 for i in range(101)]\n",
        "  label_list[index] = 1\n",
        "  label_list = np.asarray(label_list)\n",
        "  label_list = label_list[np.newaxis,...]\n",
        "  batch_labels[j] = label_list\n",
        "  \n",
        "batch_videos = np.asarray(batch_videos)\n",
        "batch_labels = np.asarray(batch_labels)\n",
        "new_model.fit(\n",
        "    x=batch_videos,\n",
        "    y=batch_labels,\n",
        "    verbose=2,\n",
        "    epochs=NUM_EPOCHS,\n",
        "    batch_size=5,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Gxh-HjOa1O7",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "test_video = download_random_video()\n",
        "video_path = fetch_ucf_video(test_video)\n",
        "sample_video = load_video(video_path)[:50]\n",
        "sample_video.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c6vHFjCcl7as",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "def new_model_predict(sample_video):\n",
        "  # Add a batch axis to the sample video.\n",
        "  new_model_input = tf.constant(sample_video, dtype=tf.float32)[tf.newaxis,...] \n",
        "\n",
        "  logits = new_model(new_model_input)[0]\n",
        "  probabilities = tf.nn.softmax(logits)\n",
        "\n",
        "  print(\"Top 5 actions:\")\n",
        "  for i in np.argsort(probabilities)[::-1][:5]:\n",
        "    actions = list(categories.keys())\n",
        "    action = actions[i]\n",
        "    print(f\"  {action:22}: {probabilities[i] * 100:5.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_7QnKVdBmA8E",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "new_model_predict(sample_video)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lspy_U85mWwk",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "to_gif(sample_video)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xd47YBRkoQZy",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
